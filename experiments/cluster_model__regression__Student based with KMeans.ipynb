{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pylab as pl\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import BayesianRidge, ARDRegression, Lasso, Ridge\n",
    "from sklearn.ensemble import BaggingRegressor, AdaBoostRegressor, RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../datasets/processed_dataset.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop([df.columns[0], df.columns[1], df.columns[2]], inplace=True, axis=1)   # dropping course details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# applying one-hot encoding on categorical features\n",
    "df = pd.concat([df, pd.get_dummies(df['Course Year'], prefix='Course Year'), pd.get_dummies(df['Department Code'], prefix='Department Code'), pd.get_dummies(df['Course Level'], prefix='Course Level'), pd.get_dummies(df['Standing'], prefix='Standing'), pd.get_dummies(df['Status'], prefix='Status')], axis=1)\n",
    "df.drop(['Course Year', 'Department Code', 'Course Level', 'Status', 'Standing'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "le.fit(['A+', 'A', 'A-', 'B+', 'B', 'B-', 'C+', 'C', 'C-', 'D+', 'D', 'D-', 'F'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(X_train, X_test):\n",
    "    X_train_cols = X_train.columns\n",
    "    X_test_cols = X_test.columns\n",
    "    sc = StandardScaler()\n",
    "    fitted_sc = sc.fit(X_train)\n",
    "    X_train_std = pd.DataFrame(fitted_sc.transform(X_train), columns=X_train_cols)\n",
    "    X_test_std = pd.DataFrame(fitted_sc.transform(X_test), columns=X_test_cols)\n",
    "    return X_train_std, X_test_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_train_data(df, train_sem, columns):\n",
    "    dataFrame = pd.DataFrame(columns=columns)\n",
    "    \n",
    "    # extracting instances from the dataset which should be in training data\n",
    "    for sem in train_sem:\n",
    "        dataFrame = pd.concat([dataFrame, df[df.iloc[:, 7] == sem]], ignore_index=True)\n",
    "    \n",
    "    X_train = dataFrame.drop('Semester', axis=1)\n",
    "    y_train = le.transform(X_train.pop('Letter Grade'))\n",
    "    return X_train, y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cluster(n_clusters, X_train, y_train, reg_model, cluster_model):\n",
    "    reg_models = {}   # {cluster_label: fitted regression model (object), ...}\n",
    "    clusters_dataset = {}  # {cluster_label: {'X': dataset (pd.DataFrame), 'y': true target values (list)}, ...}\n",
    "    one_hot_depts = list(X_train.columns[16:51])\n",
    "    \n",
    "    # fitting a clustering model based on GPA, Completed Credits and Departments\n",
    "    fitted_cluster_model = cluster_model(n_clusters=n_clusters).fit(X_train[['GPA', 'Completed Credits'] + one_hot_depts])\n",
    "    cluster_labels = fitted_cluster_model.labels_\n",
    "    \n",
    "    # splitting the main dataset into sub-dataFrames based on their cluster label\n",
    "    for i in range(len(cluster_labels)):\n",
    "        clusters_dataset.setdefault(cluster_labels[i], {})\n",
    "        clusters_dataset[cluster_labels[i]].setdefault('X', pd.DataFrame(columns=X_train.columns))\n",
    "        clusters_dataset[cluster_labels[i]].setdefault('y', [])\n",
    "\n",
    "        df_row = list(X_train.iloc[i, :])   # getting the corresponding row from main dataset\n",
    "        cluster_data = clusters_dataset[cluster_labels[i]]   # getting the corresponding cluster data structure\n",
    "        cluster_data['X'].loc[len(cluster_data['X'])] = df_row\n",
    "        cluster_data['y'].append(y_train[i])\n",
    "    \n",
    "    # fitting a regression model to each clustering and storing the fitted models\n",
    "    for cluster_label in clusters_dataset:\n",
    "        reg_models.setdefault(cluster_label, 0)\n",
    "        fitted_reg_model = reg_model.fit(clusters_dataset[cluster_label]['X'], clusters_dataset[cluster_label]['y'])\n",
    "        reg_models[cluster_label] = fitted_reg_model\n",
    "        \n",
    "    return reg_models, fitted_cluster_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cluster_test_data(cluster_model, X_test, y_test):\n",
    "    test_dfs = {}   # splitting the test dataset into sub-dataFrames based on their predicted cluster label\n",
    "    one_hot_depts = list(X_test.columns[16:51])\n",
    "    # predicting the cluster labels of test data using a cluster model fitted on the whole dataset so far\n",
    "    predicted_clusters = cluster_model.predict(X_test[['GPA', 'Completed Credits'] + one_hot_depts])\n",
    "    \n",
    "    # splitting the test dataset based on their cluster label\n",
    "    for i in range(len(predicted_clusters)):\n",
    "        test_dfs.setdefault(predicted_clusters[i], {})\n",
    "        test_dfs[predicted_clusters[i]].setdefault('X', pd.DataFrame(columns=X_test.columns))\n",
    "        test_dfs[predicted_clusters[i]].setdefault('y', [])\n",
    "        \n",
    "        df_row = list(X_test.iloc[i, :])\n",
    "        cluster_data = test_dfs[predicted_clusters[i]]\n",
    "        cluster_data['X'].loc[len(cluster_data['X'])] = df_row\n",
    "        cluster_data['y'].append(y_test[i])\n",
    "        \n",
    "    return test_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_error_score(df, columns, reg_model, cluster_model):\n",
    "    error_scores = {}   # storing error scores in a dict with shape: \n",
    "                        # {num_clusters (k=2,3,...,7): \n",
    "                          # {num_training_semesters (N=1,2,...,7): \n",
    "                              # {cluster_label: [RMSE, MAE], ...}, \n",
    "                          # ...}, \n",
    "                        #...}\n",
    "\n",
    "    sorted_semesters = sorted(set(df.iloc[:, 7]))   # sorting semesters in a time series manner\n",
    "    for num_clusters in range(10, 31, 5):\n",
    "        error_scores.setdefault(str(num_clusters), {})\n",
    "        for sem_idx in range(1, len(sorted_semesters)):\n",
    "            error_scores[str(num_clusters)].setdefault(str(sem_idx), {'y_true': [], 'y_pred': []})\n",
    "            \n",
    "            # preparing the training data from the beginning of the dataset so far\n",
    "            training_sem = sorted_semesters[:sem_idx]\n",
    "            test_sem = sorted_semesters[sem_idx]\n",
    "            X_train, y_train = get_train_data(df, training_sem, columns)\n",
    "            \n",
    "            # preparing the test dataframe\n",
    "            X_test = df[df.iloc[:, 7] == test_sem]\n",
    "            X_test.drop('Semester', axis=1, inplace=True)\n",
    "            X_test.index = range(len(X_test))\n",
    "            y_test = le.transform(X_test.pop('Letter Grade'))\n",
    "            \n",
    "            # standardizing the dataset for faster optimization\n",
    "            X_train, X_test = standardize(X_train, X_test)\n",
    "            \n",
    "            # getting the cluster model fitted on training data and each clusters' regression model in a dict\n",
    "            reg_models, fitted_cluster_model = fit_cluster(num_clusters, X_train, y_train, reg_model, cluster_model)\n",
    "            \n",
    "            # getting the clustered test data\n",
    "            test_dfs = cluster_test_data(fitted_cluster_model, X_test, y_test)\n",
    "            \n",
    "            # for each cluster label in test dataFrames:\n",
    "                # 1. retrieve the regression model fitted earlier on the same cluster label\n",
    "                # 2. predict the test data from the same cluster label with the retrieved regression model\n",
    "                \n",
    "            for cluster_label in test_dfs:\n",
    "                fitted_reg_model = reg_models[cluster_label]\n",
    "                y_true = test_dfs[cluster_label]['y']\n",
    "                y_pred = fitted_reg_model.predict(test_dfs[cluster_label]['X'])\n",
    "                y_pred = list(y_pred)\n",
    "                                \n",
    "                error_scores[str(num_clusters)][str(sem_idx)]['y_true'] += y_true\n",
    "                error_scores[str(num_clusters)][str(sem_idx)]['y_pred'] += y_pred\n",
    "                \n",
    "    return error_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_error_scores(scores, title):\n",
    "    best_scores = {}\n",
    "    for n_clusters in scores:\n",
    "        best_scores.setdefault(n_clusters, [100, 100, -100, -100])   # [rmse_train, rmse_test, r2_train, r2_test]\n",
    "        for n_training_sem in scores[n_clusters]:\n",
    "            clust_scores = scores[n_clusters][n_training_sem].values()\n",
    "            for s in clust_scores:\n",
    "                if s['train'][0] < best_scores[n_clusters][0]:\n",
    "                    best_scores[n_clusters][0] = s['train'][0]\n",
    "                elif s['train'][1] > best_scores[n_clusters][2]:\n",
    "                    best_scores[n_clusters][2] = s['train'][1]\n",
    "                elif s['test'][0] < best_scores[n_clusters][1]:\n",
    "                    best_scores[n_clusters][1] = s['test'][0]\n",
    "                elif s['test'][1] > best_scores[n_clusters][3]:\n",
    "                    best_scores[n_clusters][3] = s['test'][1]\n",
    "        \n",
    "    fig, ax = plt.subplots(figsize=(15,5))\n",
    "    x = np.arange(2,8)\n",
    "    ax.plot(x, [best_scores[k][0] for k in sorted(best_scores)], marker='o', label='RMSE train', linestyle='--', linewidth=3, mew=3)\n",
    "    ax.plot(x, [best_scores[k][1] for k in sorted(best_scores)], marker='x', label='RMSE test', linestyle='-', linewidth=3, mew=3)\n",
    "    ax.plot(x, [best_scores[k][2] for k in sorted(best_scores)], marker='v', label='R\\u00b2 train', linestyle='dotted', linewidth=3, mew=3)\n",
    "    ax.plot(x, [best_scores[k][3] for k in sorted(best_scores)], marker='d', label='R\\u00b2 test', linestyle='dashdot', linewidth=3, mew=3)\n",
    "    \n",
    "    ax.set_title(title, fontfamily='serif', fontsize=20)\n",
    "    ax.set_yticklabels([round(i,1) for i in ax.get_yticks()], fontfamily='serif', fontsize=20)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(['k={}'.format(i) for i in range(2, 8)], fontfamily='serif', fontsize=20)\n",
    "    ax.set_xlabel('Number of Clusters (k)', fontsize=20, fontfamily='serif')\n",
    "    ax.set_ylabel('Error', fontsize=20, fontfamily='serif')\n",
    "    ax.grid(True)\n",
    "    ax.legend(prop={'size': 20, 'family': 'serif'}, loc='center', bbox_to_anchor=(0.5, -0.25), ncol=4)\n",
    "    \n",
    "#     plt.savefig('sample.svg', bbox='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../hyperparameters/tuned_hyperparams (student based).json') as fr:\n",
    "    tuned_hyperparams = json.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_errors = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/clustering_student_based_regression_results (with KMeans).json') as fr:\n",
    "    results = json.load(fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = tuned_hyperparams['SVR']\n",
    "errors = get_error_score(df, columns, SVR(C=hps['C'], kernel=hps['kernel'], epsilon=hps['epsilon']), KMeans)\n",
    "model_errors['SVR'] = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_scores(results['SVR'], 'SupportVectorRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = tuned_hyperparams['BayesianRidge']\n",
    "errors = get_error_score(df, columns, BayesianRidge(lambda_1=hps['lambda_1'], lambda_2=hps['lambda_2'],\n",
    "                                                    alpha_1=hps['alpha_1'], alpha_2=hps['alpha_2']), KMeans)\n",
    "model_errors['BayesianRidge'] = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_scores(results['BayesianRidge'], 'BayesianRidge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = tuned_hyperparams['Lasso']\n",
    "errors = get_error_score(df, columns, Lasso(alpha=hps['alpha']), KMeans)\n",
    "model_errors['Lasso'] = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_scores(results['Lasso'], 'Lasso')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = tuned_hyperparams['Ridge']\n",
    "errors = get_error_score(df, columns, Ridge(alpha=hps['alpha']), KMeans)\n",
    "model_errors['Ridge'] = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_scores(results['Ridge'], 'Ridge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = tuned_hyperparams['BaggingRegressor']\n",
    "errors = get_error_score(df, columns, BaggingRegressor(n_estimators=hps['n_estimators']), KMeans)\n",
    "model_errors['BaggingRegressor'] = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_scores(results['BaggingRegressor'], 'BaggingRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = tuned_hyperparams['AdaBoostRegressor']\n",
    "scores = get_error_score(df, columns, AdaBoostRegressor(n_estimators=hps['n_estimators'], learning_rate=hps['learning_rate']), KMeans)\n",
    "model_errors['AdaBoostRegressor'] = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_scores(results['AdaBoostRegressor'], 'AdaBoostRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = tuned_hyperparams['RandomForestRegressor']\n",
    "errors = get_error_score(df, columns, RandomForestRegressor(n_estimators=hps['n_estimators']), KMeans)\n",
    "model_errors['RandomForestRegressor'] = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_scores(results['RandomForestRegressor'], 'RandomForestRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hps = tuned_hyperparams['GradientBoostingRegressor']\n",
    "errors = get_error_score(df, columns, GradientBoostingRegressor(learning_rate=hps['learning_rate'], loss=hps['loss'],\n",
    "                                                                n_estimators=hps['n_estimators'], max_depth=hps['max_depth']), KMeans)\n",
    "model_errors['GradientBoostingRegressor'] = errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_error_scores(results['GradientBoostingRegressor'], 'GradientBoostingRegressor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('clustering_student_based_regression_results (with KMeans).json', 'w') as fw:\n",
    "    json.dump(model_errors, fw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
